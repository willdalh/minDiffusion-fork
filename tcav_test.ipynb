{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_concepts(x, labels, label_of_interest, index_manager, device) -> float:\n",
    "    y = labels == label_of_interest\n",
    "\n",
    "    # Shuffle\n",
    "    # perm = torch.randperm(n)\n",
    "    # samples = samples[perm]\n",
    "    # labels = labels[perm]\n",
    "    # x = x[perm]\n",
    "    # y = y[perm]\n",
    "\n",
    "    # n_label_of_interest = (labels == label_of_interest).cpu().sum().item()\n",
    "    # indices = torch.nonzero(y).squeeze(1)\n",
    "    # non_indices = torch.nonzero(~y).squeeze(1)\n",
    "    # x = torch.cat([x[indices], x[non_indices][:n_label_of_interest]]).to(device)\n",
    "    # y = torch.cat([y[indices], y[non_indices][:n_label_of_interest]]).to(device)\n",
    "    # samples = torch.cat([samples[indices], samples[non_indices][:n_label_of_interest]]).to(device)\n",
    "    # labels = torch.cat([labels[indices], labels[non_indices][:n_label_of_interest]]).to(device)\n",
    "\n",
    "    x = x[index_manager[label_of_interest]]\n",
    "    y = y[index_manager[label_of_interest]]\n",
    "    labels = labels[index_manager[label_of_interest]]\n",
    "\n",
    "    # print(f\"Number of samples: {x.shape[0]}\")\n",
    "\n",
    "    test_size = 0.25\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=745)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(x_train.reshape(x_train.shape[0], -1).cpu(), y_train.cpu())\n",
    "    accuracy = clf.score(x_test.reshape(x_test.shape[0], -1).cpu(), y_test.cpu())\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the diffusion model\n",
    "model = DDPM(eps_model=DummyEpsModel(1), betas=(1e-4, 0.02), n_T=1000)\n",
    "model.load_state_dict(torch.load(\"./contents/ddpm_mnist.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "dataset_name = \"980\"\n",
    "\n",
    "# Load samples, labels and seeds\n",
    "dataset = torch.load(f\"./datasets/{dataset_name}_samples.pth\", map_location=device)\n",
    "labels = torch.load(f\"./datasets/{dataset_name}_labels.pth\", map_location=device)\n",
    "seed = torch.load(f\"./datasets/{dataset_name}_seed.pth\", map_location=device)\n",
    "\n",
    "n = dataset.shape[0]\n",
    "# samples = dataset[:, 0][:, None, ...]\n",
    "original_noise = dataset[:, 1][:, None, ...]\n",
    "\n",
    "\n",
    "whole_pipeline = []\n",
    "for m in model.eps_model.modules():\n",
    "    if not isinstance(m, torch.nn.Sequential) and not isinstance(m, DummyEpsModel):\n",
    "        whole_pipeline.append(m)\n",
    "whole_net = torch.nn.Sequential(*whole_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = list(range(1000, 0, -1))\n",
    "digits_to_test = [3]\n",
    "test_every = 100\n",
    "logging_dir = f\"./tcav_results/seeded_test\"\n",
    "if not isdir(logging_dir):\n",
    "    os.mkdir(logging_dir)\n",
    "\n",
    "layers_to_inspect_indices = [i for i, m in enumerate(whole_net) if isinstance(m, nn.Conv2d)]\n",
    "\n",
    "\n",
    "# Create index manager, used when we create the datasets for each label of interest\n",
    "index_manager = {}\n",
    "for d in digits_to_test:\n",
    "    is_label = labels == d\n",
    "    n_digits = (is_label).cpu().sum().item()\n",
    "    present_indices = torch.nonzero(is_label).squeeze(1)\n",
    "    absent_indices = torch.nonzero(~is_label).squeeze(1)\n",
    "    indices = torch.cat([present_indices, absent_indices[:n_digits]]).to(device)\n",
    "    index_manager[d] = indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "x_t = original_noise\n",
    "\n",
    "with torch.no_grad():\n",
    "    rng = torch.manual_seed(seed)\n",
    "    _ = torch.randn(n, *(1, 28, 28)).to(device) # Continue RNG state\n",
    "    # Loop for T..1\n",
    "    for t in steps:\n",
    "        print(f\"Step: {t}\")\n",
    "        z = torch.randn(n, *(1, 28, 28)).to(device) if t > 1 else 0\n",
    "        eps = x_t.clone()\n",
    "            \n",
    "        if t%test_every == 0 or t==1: # Apply each layer individually\n",
    "            print(f\"Testing at {t}\")\n",
    "            for i, layer in enumerate(whole_pipeline):\n",
    "                \n",
    "                eps = layer(eps) \n",
    "                if i in layers_to_inspect_indices:\n",
    "                    # Suspend RNG\n",
    "                    curr_rng_state = rng.get_state()\n",
    "                    print(f\"Testing layer {i}\")\n",
    "\n",
    "                    for label_of_interest in digits_to_test:\n",
    "                        #Identify concepts here\n",
    "                        x = x[index_manager[label_of_interest]]\n",
    "                        y = y[index_manager[label_of_interest]]\n",
    "                        labels = labels[index_manager[label_of_interest]]\n",
    "\n",
    "                        # print(f\"Number of samples: {x.shape[0]}\")\n",
    "\n",
    "                        test_size = 0.25\n",
    "                        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=745)\n",
    "                        clf = LogisticRegression()\n",
    "                        clf.fit(x_train.reshape(x_train.shape[0], -1).cpu(), y_train.cpu())\n",
    "                        accuracy = clf.score(x_test.reshape(x_test.shape[0], -1).cpu(), y_test.cpu())\n",
    "                        results_list.append([t, i, label_of_interest, accuracy])\n",
    "\n",
    "                    # Resume RNG\n",
    "                    rng.set_state(curr_rng_state)\n",
    "        else: # Apply the whole network\n",
    "            eps = whole_net(x_t)\n",
    "\n",
    "\n",
    "        x_t = (model.oneover_sqrta[t] * (x_t - eps * model.mab_over_sqrtmab[t]) + model.sqrt_beta_t[t] * z)\n",
    "\n",
    "print(x_t.shape)\n",
    "# Plot the samples\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 4))\n",
    "for i, ax in enumerate(np.array(list(axes)).T):\n",
    "    ax.imshow(x_t[i].mean(dim=0).reshape(28, 28), cmap=\"gray\")\n",
    "    # ax.set_title(f\"Label: {predicted}\")\n",
    "    # ax.imshow(samples_test[i].reshape(28, 28), cmap=\"gray\")\n",
    "# Save the figure\n",
    "fig.savefig(f\"{logging_dir}/{dataset_name}_samples.png\")\n",
    "\n",
    "\n",
    "# results_list to dataframe with columns: t, layer, digit_separated, accuracy and indices\n",
    "\n",
    "results = pd.DataFrame(results_list, columns=[\"t\", \"layer\", \"digit_separated\", \"accuracy\"])\n",
    "results.index.name = \"id\"\n",
    "results.to_csv(f\"{logging_dir}/{dataset_name}_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
