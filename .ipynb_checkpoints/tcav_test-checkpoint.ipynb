{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genericpath import isdir\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from superminddpm import DDPM, DummyEpsModel\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_concepts(x, labels, label_of_interest, index_manager, device) -> float:\n",
    "    y = labels == label_of_interest\n",
    "\n",
    "    # Shuffle\n",
    "    # perm = torch.randperm(n)\n",
    "    # samples = samples[perm]\n",
    "    # labels = labels[perm]\n",
    "    # x = x[perm]\n",
    "    # y = y[perm]\n",
    "\n",
    "    # n_label_of_interest = (labels == label_of_interest).cpu().sum().item()\n",
    "    # indices = torch.nonzero(y).squeeze(1)\n",
    "    # non_indices = torch.nonzero(~y).squeeze(1)\n",
    "    # x = torch.cat([x[indices], x[non_indices][:n_label_of_interest]]).to(device)\n",
    "    # y = torch.cat([y[indices], y[non_indices][:n_label_of_interest]]).to(device)\n",
    "    # samples = torch.cat([samples[indices], samples[non_indices][:n_label_of_interest]]).to(device)\n",
    "    # labels = torch.cat([labels[indices], labels[non_indices][:n_label_of_interest]]).to(device)\n",
    "\n",
    "    x = x[index_manager[label_of_interest]]\n",
    "    y = y[index_manager[label_of_interest]]\n",
    "    labels = labels[index_manager[label_of_interest]]\n",
    "\n",
    "    # print(f\"Number of samples: {x.shape[0]}\")\n",
    "\n",
    "    test_size = 0.25\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=745)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(x_train.reshape(x_train.shape[0], -1).cpu(), y_train.cpu())\n",
    "    accuracy = clf.score(x_test.reshape(x_test.shape[0], -1).cpu(), y_test.cpu())\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the diffusion model\n",
    "model = DDPM(eps_model=DummyEpsModel(1), betas=(1e-4, 0.02), n_T=1000)\n",
    "model.load_state_dict(torch.load(\"./contents/ddpm_mnist.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "dataset_name = \"980\"\n",
    "\n",
    "# Load samples, labels and seeds\n",
    "dataset = torch.load(f\"./datasets/{dataset_name}_samples.pth\", map_location=device)\n",
    "labels = torch.load(f\"./datasets/{dataset_name}_labels.pth\", map_location=device)\n",
    "seed = torch.load(f\"./datasets/{dataset_name}_seed.pth\", map_location=device)\n",
    "\n",
    "n = dataset.shape[0]\n",
    "# samples = dataset[:, 0][:, None, ...]\n",
    "original_noise = dataset[:, 1][:, None, ...]\n",
    "\n",
    "\n",
    "whole_pipeline = []\n",
    "for m in model.eps_model.modules():\n",
    "    if not isinstance(m, torch.nn.Sequential) and not isinstance(m, DummyEpsModel):\n",
    "        whole_pipeline.append(m)\n",
    "whole_net = torch.nn.Sequential(*whole_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = list(range(1000, 0, -1))\n",
    "digits_to_test = [3]\n",
    "test_every = 100\n",
    "logging_dir = f\"./tcav_results/seeded_test\"\n",
    "if not isdir(logging_dir):\n",
    "    os.mkdir(logging_dir)\n",
    "\n",
    "layers_to_inspect_indices = [i for i, m in enumerate(whole_net) if isinstance(m, nn.Conv2d)]\n",
    "\n",
    "\n",
    "# Create index manager, used when we create the datasets for each label of interest\n",
    "index_manager = {}\n",
    "for d in digits_to_test:\n",
    "    is_label = labels == d\n",
    "    n_digits = (is_label).cpu().sum().item()\n",
    "    present_indices = torch.nonzero(is_label).squeeze(1)\n",
    "    absent_indices = torch.nonzero(~is_label).squeeze(1)\n",
    "    indices = torch.cat([present_indices, absent_indices[:n_digits]]).to(device)\n",
    "    index_manager[d] = indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000\n",
      "Testing at 1000\n",
      "Testing layer 0\n",
      "Testing layer 3\n",
      "Testing layer 6\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.47 GiB (GPU 0; 15.90 GiB total capacity; 826.44 MiB already allocated; 13.49 GiB free; 1.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-40237e0ffb2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhole_pipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers_to_inspect_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0;31m# Suspend RNG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 453\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    454\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.47 GiB (GPU 0; 15.90 GiB total capacity; 826.44 MiB already allocated; 13.49 GiB free; 1.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "x_t = original_noise\n",
    "\n",
    "with torch.no_grad():\n",
    "    rng = torch.manual_seed(seed)\n",
    "    _ = torch.randn(n, *(1, 28, 28)).to(device) # Continue RNG state\n",
    "    # Loop for T..1\n",
    "    for t in steps:\n",
    "        print(f\"Step: {t}\")\n",
    "        z = torch.randn(n, *(1, 28, 28)).to(device) if t > 1 else 0\n",
    "        eps = x_t.clone()\n",
    "            \n",
    "        if t%test_every == 0 or t==1: # Apply each layer individually\n",
    "            print(f\"Testing at {t}\")\n",
    "            for i, layer in enumerate(whole_pipeline):\n",
    "                \n",
    "                eps = layer(eps) \n",
    "                if i in layers_to_inspect_indices:\n",
    "                    # Suspend RNG\n",
    "                    curr_rng_state = rng.get_state()\n",
    "                    print(f\"Testing layer {i}\")\n",
    "\n",
    "                    for label_of_interest in digits_to_test:\n",
    "                        #Identify concepts here\n",
    "                        x = x[index_manager[label_of_interest]]\n",
    "                        y = y[index_manager[label_of_interest]]\n",
    "                        labels = labels[index_manager[label_of_interest]]\n",
    "\n",
    "                        # print(f\"Number of samples: {x.shape[0]}\")\n",
    "\n",
    "                        test_size = 0.25\n",
    "                        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=745)\n",
    "                        clf = LogisticRegression()\n",
    "                        clf.fit(x_train.reshape(x_train.shape[0], -1).cpu(), y_train.cpu())\n",
    "                        accuracy = clf.score(x_test.reshape(x_test.shape[0], -1).cpu(), y_test.cpu())\n",
    "                        results_list.append([t, i, label_of_interest, accuracy])\n",
    "\n",
    "                    # Resume RNG\n",
    "                    rng.set_state(curr_rng_state)\n",
    "        else: # Apply the whole network\n",
    "            eps = whole_net(x_t)\n",
    "\n",
    "\n",
    "        x_t = (model.oneover_sqrta[t] * (x_t - eps * model.mab_over_sqrtmab[t]) + model.sqrt_beta_t[t] * z)\n",
    "\n",
    "print(x_t.shape)\n",
    "# Plot the samples\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 4))\n",
    "for i, ax in enumerate(np.array(list(axes)).T):\n",
    "    ax.imshow(x_t[i].mean(dim=0).reshape(28, 28), cmap=\"gray\")\n",
    "    # ax.set_title(f\"Label: {predicted}\")\n",
    "    # ax.imshow(samples_test[i].reshape(28, 28), cmap=\"gray\")\n",
    "# Save the figure\n",
    "fig.savefig(f\"{logging_dir}/{dataset_name}_samples.png\")\n",
    "\n",
    "\n",
    "# results_list to dataframe with columns: t, layer, digit_separated, accuracy and indices\n",
    "\n",
    "results = pd.DataFrame(results_list, columns=[\"t\", \"layer\", \"digit_separated\", \"accuracy\"])\n",
    "results.index.name = \"id\"\n",
    "results.to_csv(f\"{logging_dir}/{dataset_name}_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
